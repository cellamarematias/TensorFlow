# ğŸ§± Â¿QuÃ© son las Capas Ocultas (Hidden Layers) en una red neuronal?

Las **capas ocultas** son los **bloques intermedios** entre la capa de entrada (input) y la capa de salida (output) de una red neuronal. Son las responsables de **aprender representaciones complejas** de los datos.

# ğŸ§  Â¿QuÃ© hacen realmente?

Cada capa oculta:
- Toma los datos de la capa anterior.
- Los **transforma aplicando una funciÃ³n matemÃ¡tica** (operaciÃ³n lineal + funciÃ³n de activaciÃ³n).
- Pasa el resultado a la siguiente capa.

AsÃ­, capa tras capa, la red puede **aprender relaciones cada vez mÃ¡s abstractas**.

Por ejemplo:
- Primeras capas podrÃ­an detectar lÃ­neas o bordes (en imÃ¡genes).
- Capas mÃ¡s profundas podrÃ­an detectar formas completas o patrones complejos.

# âš™ï¸ Â¿QuÃ© contiene una capa oculta?

Una capa oculta tiene:
- **Neuronas (unidades)**: cada una con sus propios **pesos** y **bias**.
- Una **funciÃ³n de activaciÃ³n**, que decide si la neurona "dispara" o no (como una puerta lÃ³gica).

La operaciÃ³n matemÃ¡tica bÃ¡sica de cada neurona es:

\[
output = \text{activation}(w_1 \cdot x_1 + w_2 \cdot x_2 + ... + b)
\]

Donde:
- \(x_i\) son las entradas
- \(w_i\) los pesos
- \(b\) el bias
- activation puede ser `relu`, `sigmoid`, `tanh`, etc.

# ğŸ”¥ Â¿QuÃ© funciones de activaciÃ³n se usan?

Las mÃ¡s comunes:
- `ReLU` (Rectified Linear Unit): muy usada en hidden layers.
- `Sigmoid`: aplasta valores entre 0 y 1 (menos usada en capas ocultas).
- `Tanh`: valores entre -1 y 1.

Ejemplo:

```
tf.keras.layers.Dense(64, activation='relu')
```

# ğŸ§ª Â¿CÃ³mo se agregan capas ocultas?

Ejemplo de red con 2 capas ocultas:

```
model = tf.keras.Sequential([
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dense(1)  # Capa de salida
])
```

# ğŸ¯ Â¿CuÃ¡ntas capas y neuronas usar?

- No hay una respuesta Ãºnica.
- Se eligen probando (experimentaciÃ³n).
- Muy pocas capas â†’ el modelo puede ser demasiado simple.
- Demasiadas capas â†’ puede sobreajustar (memorizar sin generalizar).

# ğŸ“Œ En resumen:

- Las **capas ocultas** permiten que la red **aprenda patrones complejos**.
- Cuantas mÃ¡s capas y neuronas, mÃ¡s poder de representaciÃ³n (pero tambiÃ©n mÃ¡s riesgo de sobreentrenar).
- Son **esenciales** para que las redes neuronales resuelvan tareas no lineales y profundas.
