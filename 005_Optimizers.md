# âš™ï¸ Â¿QuÃ© es un Optimizer (Optimizador) en una red neuronal?

Un **optimizador** es el algoritmo que se encarga de **ajustar los pesos internos** del modelo para **minimizar la pÃ©rdida (loss)** durante el entrenamiento.

# ğŸ§  Â¿CÃ³mo funciona?
DespuÃ©s de cada pasada por los datos (Ã©poca), el optimizador **modifica los pesos** del modelo basÃ¡ndose en el error que obtuvo. El objetivo es que la **pÃ©rdida sea cada vez menor**.

# ğŸ“¦ Optimizers comunes en TensorFlow/Keras

## âœ… `SGD` - Stochastic Gradient Descent
- El mÃ¡s simple.
- Actualiza los pesos con cada muestra o mini-lote.
- Puede ser lento y sensible a la escala de los datos.

## ğŸš€ `Adam` - Adaptive Moment Estimation
- Uno de los mÃ¡s usados.
- Combina lo mejor de `SGD` + tÃ©cnicas como momento y tasa de aprendizaje adaptativa.
- Generalmente **requiere menos ajuste manual**.
- Muy eficiente para muchos tipos de problemas.

## ğŸŒ€ Otros:
- `RMSprop`: Ãºtil en redes recurrentes (RNN).
- `Adagrad`, `Adadelta`: se adaptan segÃºn el parÃ¡metro.

# ğŸ§ª Â¿CÃ³mo lo usÃ¡s?
Ejemplo en cÃ³digo:

```
model.compile(
  loss='mae',
  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
  metrics=['mae']
)
```

# ğŸ“Œ En resumen:
- El optimizador **actualiza los pesos** del modelo para que **aprenda mejor**.
- Elegir el correcto puede **mejorar el entrenamiento**.
- `Adam` es una **gran elecciÃ³n por defecto** para comenzar.
